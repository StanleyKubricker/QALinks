{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c332aa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/david/Desktop/QA Media/Models/tokenizer_config.json',\n",
       " '/Users/david/Desktop/QA Media/Models/special_tokens_map.json',\n",
       " '/Users/david/Desktop/QA Media/Models/vocab.txt',\n",
       " '/Users/david/Desktop/QA Media/Models/added_tokens.json',\n",
       " '/Users/david/Desktop/QA Media/Models/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Specify the directory path where you want to save the model and tokenizer\n",
    "save_directory = \"/Users/david/Desktop/QA Media/Models\"\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e52c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "articles = {}\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel('article urls.xlsx')\n",
    "\n",
    "main_url = \"https://www.qa-financial.com\"\n",
    "\n",
    "# Convert the DataFrame to a list\n",
    "urls = df['URL'].tolist()  # Replace 'urls' with the actual column name in the Excel file\n",
    "\n",
    "# For each URL, get the text content\n",
    "for url in urls:\n",
    "    try:\n",
    "        response = requests.get(main_url + url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Get the title and text based on your description\n",
    "        #title = soup.find('title').get_text()\n",
    "        text = soup.find('meta', attrs={'name': 'description'}).get('content')\n",
    "\n",
    "        articles[main_url + url] = text\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "import json\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('dict_file.json', 'w') as file:\n",
    "    json.dump(articles, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce04fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content = [x for x in articles.values()]\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "\n",
    "# Assume `documents` is a list of your news articles\n",
    "embeddings = [get_embeddings(doc) for doc in article_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe5b970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the list of arrays.\n",
    "with open('arrays.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
